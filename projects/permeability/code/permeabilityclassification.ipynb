{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "#import rasterio as rst\n",
    "from osgeo import gdal\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage import data, transform\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.morphology import disk\n",
    "from skimage.filters import thresholding,rank\n",
    "import rasterio as rio\n",
    "from scipy import stats\n",
    "import datetime\n",
    "from skimage.morphology import rectangle,square\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful functions used in classification, image IO, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of \"random\" colors (for a nicer output)\n",
    "COLORS = [\"#000000\", \"#FFFF00\"]# , \"#1CE6FF\", \"#FF34FF\", \"#FF4A46\", \"#008941\"]\n",
    "\n",
    "\n",
    "def create_mask_from_vector(vector_data_path, cols, rows, geo_transform,\n",
    "                            projection, target_value=1):\n",
    "    \"\"\"Rasterize the given vector (wrapper for gdal.RasterizeLayer).\"\"\"\n",
    "    data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "    layer = data_source.GetLayer(0)\n",
    "    driver = gdal.GetDriverByName('MEM')  # In memory dataset\n",
    "    target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    gdal.RasterizeLayer(target_ds, [1], layer, burn_values=[target_value])\n",
    "    return target_ds\n",
    "\n",
    "\n",
    "def vectors_to_raster(file_paths, rows, cols, geo_transform, projection):\n",
    "    \"\"\"Rasterize the vectors in the given directory in a single image.\"\"\"\n",
    "    labeled_pixels = np.zeros((rows, cols))\n",
    "    for i, path in enumerate(file_paths):\n",
    "        label = i+1\n",
    "        ds = create_mask_from_vector(path, cols, rows, geo_transform, projection, target_value=label)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        labeled_pixels += band.ReadAsArray()\n",
    "        ds = None\n",
    "    return labeled_pixels\n",
    "\n",
    "\n",
    "def write_geotiff(fname, data, geo_transform, projection):\n",
    "    \"\"\"Create a GeoTIFF file with the given data.\"\"\"\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    dataset = None  # Close the file\n",
    "\n",
    "#sys.path.append('/media/nero/DATADRIVE1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses a training dataset, consiting of folders of one-per-image and two shapefiles (perm/imperm) to create a binary permeability classifier. This can be updated to use the city-index with \"type\" field on the zoneing information so we can better train classifiers for each urban zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "# Allow division by zero\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "working_directory = '/media/DATADRIVE/permability/train'\n",
    "for root, dirs, files in os.walk(working_directory, topdown=True):\n",
    "    shapefiles = [os.path.join(root,f)  for f in files if f.endswith('.shp')]\n",
    "    image =  [os.path.join(root,f)  for f in files if f.endswith('.tif')]\n",
    "    if shapefiles and image:#e  and  root.startswith('M'):\n",
    "        print(root,image)\n",
    "        raster_dataset = gdal.Open(image[0], gdal.GA_ReadOnly)\n",
    "        \n",
    "        geo_transform = raster_dataset.GetGeoTransform()\n",
    "        proj = raster_dataset.GetProjectionRef()\n",
    "        bands_data = []\n",
    "        \n",
    "        for b in range(1, raster_dataset.RasterCount+1):\n",
    "            band = raster_dataset.GetRasterBand(b)\n",
    "            bands_data.append(band.ReadAsArray())\n",
    "#         b4 = bands_data[3]\n",
    "#         b3 = bands_data[0]\n",
    "#        #bands_data = np.dstack(bands_data)\n",
    "\n",
    "#         ndvi = ((b4.astype(np.float32) - b3.astype(np.float32)) / (b4 + b3))*100\n",
    "#         bands_data.append(ndvi)\n",
    "#         bands_data.pop(0)\n",
    "#         bands_data.pop(1)\n",
    "#        bands_data.pop(3)\n",
    "     #   print(bands_data)\n",
    "        bands_data = np.dstack(bands_data)\n",
    "        bands_data = np.nan_to_num(bands_data)\n",
    "        rows, cols, n_bands = bands_data.shape\n",
    "        print(\"Training: \"+root)\n",
    "        labeled_pixels = vectors_to_raster(shapefiles, rows, cols, geo_transform,proj)\n",
    "        is_train = np.nonzero(labeled_pixels)\n",
    "        training_labels = labeled_pixels[is_train]\n",
    "        training_samples = bands_data[is_train]\n",
    "        classifier.fit(training_samples, training_labels)\n",
    "#saves trained classifier to be reused later\n",
    "f = open('/media/DATADRIVE/permability/ndf_perm_classifier_%.pkl'%,'wb')\n",
    "pickle.dump(classifier,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification on unseen images. Loops through a working_directory, creates ndvi band, and classifies permeability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#classifier = pickle.load(open('/media/DATADRIVE/imagery/suburban_a_ndfclassifier_022618.pkl','rb'))\n",
    "#this loads pre-trained model. We'd Ideally have one for each type of urban zone - e.g., industrial etc\n",
    "#classifier = pickle.load(open('/media/DATADRIVE/permability/ndf_perm_classifier_181118.pkl','rb'))\n",
    "working_directory = '/media/DATADRIVE/imagery/raw_tiffs_2017/'\n",
    "for root, dirs, files in os.walk(working_directory, topdown=True):\n",
    "    for f in files:\n",
    "        outfile = f.split('.')[0]+'_9classified.tif'\n",
    "        if f.endswith('.tif') and not os.path.isfile(os.path.join('/media/DATADRIVE/permability/out_tifs/',outfile)):\n",
    "            print(os.path.join(root,f))\n",
    "            raster_dataset = gdal.Open(os.path.join(root,f), gdal.GA_ReadOnly)\n",
    "            geo_transform = raster_dataset.GetGeoTransform()\n",
    "            proj = raster_dataset.GetProjectionRef()\n",
    "            bands_data = []\n",
    "            for b in range(1, raster_dataset.RasterCount+1):\n",
    "                band = raster_dataset.GetRasterBand(b)\n",
    "                bands_data.append(band.ReadAsArray())\n",
    "            b4 = bands_data[3]\n",
    "            b3 = bands_data[0]\n",
    "            ndvi = ((b4.astype(np.float32) - b3.astype(np.float32)) / (b4 + b3))*100\n",
    "            bands_data.append(ndvi)\n",
    "#             bands_data.pop(0)\n",
    "#             bands_data.pop(1)\n",
    "#             bands_data.pop(2)\n",
    "            bands_data = np.dstack(bands_data)\n",
    "            rows, cols, n_bands = bands_data.shape\n",
    "            n_samples = rows*cols\n",
    "            flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "            classifier.n_jobs=-1\n",
    "            result = classifier.predict(np.nan_to_num(flat_pixels))\n",
    "            classification = result.reshape((rows, cols))\n",
    "#             classification3 =rank.modal(classification.astype(np.int8), square(3))\n",
    "#             outfile = f.split('.')[0]+'_3classified.tif'\n",
    "#             write_geotiff('./workspace/classified_tiffs/'+outfile, classification3, geo_transform, proj)\n",
    "#             classification9 =rank.modal(classification.astype(np.int8), square(9))\n",
    "#             outfile = image[0].split('.')[0]+'_9classified.tif'\n",
    "#             write_geotiff(root+outfile, classification9, geo_transform, proj)\n",
    "            classification12 =rank.modal(classification.astype(np.int8), square(9))\n",
    "            #classification12[classification12 >2] = 1 #any value greater than 2 is assigned 1 -impermeable\n",
    "            #classification12[classification12 <= 2] = 0 #any value with 2 are assigned 0 - permeable\n",
    "            print(classification12)\n",
    "            write_geotiff('/media/DATADRIVE/permability/out_tifs/'+outfile, classification12, geo_transform, proj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classify function to run if we want do multiprocessing. Note there was some errors with loading the individual modeals in each thread and benched this approach for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(fs):\n",
    "#     classifier = pickle.load(open('/media/DATADRIVE/imagery/suburban_a_ndfclassifier_022618.pkl','rb'))\n",
    "    for f in fs:\n",
    "        root = '/media/DATADRIVE/imagery/raw_tiffs_2017/'\n",
    "        print(os.path.join(root,f))\n",
    "        raster_dataset = gdal.Open(os.path.join(root,f), gdal.GA_ReadOnly)\n",
    "        print(f)\n",
    "        geo_transform = raster_dataset.GetGeoTransform()\n",
    "        proj = raster_dataset.GetProjectionRef()\n",
    "        bands_data = []\n",
    "        for b in range(1, raster_dataset.RasterCount+1):\n",
    "            band = raster_dataset.GetRasterBand(b)\n",
    "            bands_data.append(band.ReadAsArray())\n",
    "#         b4 = bands_data[3]\n",
    "#         b3 = bands_data[0]\n",
    "\n",
    "            ndvi = ((b4.astype(np.float32) - b3.astype(np.float32)) / (b4 + b3))*100\n",
    "            bands_data.append(ndvi)\n",
    "#             bands_data.pop(0)\n",
    "#             bands_data.pop(1)\n",
    "#        bands_data.pop(3)\n",
    "        bands_data = np.dstack(bands_data)\n",
    "        rows, cols, n_bands = bands_data.shape\n",
    "        n_samples = rows*cols\n",
    "        flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "        classifier.n_jobs=2\n",
    "        result = classifier.predict(np.nan_to_num(flat_pixels))\n",
    "        classification = result.reshape((rows, cols))\n",
    "#             classification3 =rank.modal(classification.astype(np.int8), square(3))\n",
    "#             outfile = f.split('.')[0]+'_3classified.tif'\n",
    "#             write_geotiff('./workspace/classified_tiffs/'+outfile, classification3, geo_transform, proj)\n",
    "#             classification9 =rank.modal(classification.astype(np.int8), square(9))\n",
    "#             outfile = image[0].split('.')[0]+'_9classified.tif'\n",
    "#             write_geotiff(root+outfile, classification9, geo_transform, proj)\n",
    "        classification12 =rank.modal(classification.astype(np.int8), square(9))\n",
    "        outfile = f.split('.')[0]+'_9classified.tif'\n",
    "        print('/media/DATADRIVE/permability/out_tifs/'+outfile)\n",
    "        write_geotiff('/media/DATADRIVE/permability/out_tifs/'+outfile, classification12, geo_transform, proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "fs = []\n",
    "working_directory = '/media/DATADRIVE/imagery/raw_tiffs_2017/'\n",
    "for root, dirs, files in os.walk(working_directory, topdown=True):\n",
    "    for f in files:\n",
    "        if f.endswith('.tif'):\n",
    "            fs.append(f)\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(6)\n",
    "    for res in pool.map(classify_images,chunks(fs,500)):\n",
    "        print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
